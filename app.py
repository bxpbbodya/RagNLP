import gradio as gr
from rag_pipeline import RAGPipeline

rag = RAGPipeline()

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏ –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤ ---
PROVIDERS = {
    "Groq": {
        "base_url": "https://api.groq.com/openai/v1",
        "model": "llama-3.3-70b-versatile",
        "note": "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è Groq API key."
    },
    "OpenAI": {
        "base_url": "https://api.openai.com/v1",
        "model": "gpt-4o-mini",
        "note": "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è OpenAI API key."
    },
    "Custom": {
        "base_url": "https://api.openai.com/v1",
        "model": "gpt-4o-mini",
        "note": "OpenAI-compatible endpoint. Base URL –≤–≤–æ–¥–∏—Ç—å—Å—è –≤—Ä—É—á–Ω—É."
    }
}

DOMAIN_TEXT = """
# üìö RAG Question Answering (NLP)

–¶–µ –¥–µ–º–æ RAG (Retrieval-Augmented Generation) —Å–∏—Å—Ç–µ–º–∏ Question Answering –Ω–∞ –±–∞–∑—ñ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ **–∑ —Ç–µ–º NLP —Ç–∞ RAG**.

**–ü–æ–∫—Ä–∏—Ç—ñ —Ç–µ–º–∏:**
- Tokenization (BPE / WordPiece / char-level)
- Word Embeddings (Word2Vec, GloVe, cosine similarity)
- Transformers (self-attention, Q/K/V, positional encoding)
- Evaluation Metrics (F1, BLEU, ROUGE)
- Overfitting, Data leakage, Train/Val/Test split
- Prompting —Ç–∞ RAG basics

**–ü—Ä–∏–∫–ª–∞–¥–∏ –∑–∞–ø–∏—Ç—ñ–≤:**
- ‚Äú–ü–æ—è—Å–Ω–∏ self-attention: —â–æ —Ç–∞–∫–µ Q, K, V?‚Äù
- ‚Äú–ß–∏–º BLEU –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –≤—ñ–¥ ROUGE?‚Äù
- ‚Äú–©–æ —Ç–∞–∫–µ data leakage —ñ —è–∫ —É–Ω–∏–∫–∞—Ç–∏?‚Äù
- ‚Äú–©–æ —Ç–∞–∫–µ cosine similarity —ñ –Ω–∞–≤—ñ—â–æ –≤–æ–Ω–∞ –¥–ª—è embeddings?‚Äù
- ‚Äú–Ø–∫—ñ —Ç–∏–ø–æ–≤—ñ –ø—Ä–æ–≤–∞–ª–∏ RAG —ñ —è–∫ —ó—Ö –∑–º–µ–Ω—à—É–≤–∞—Ç–∏?‚Äù

–Ø–∫—â–æ –ø–∏—Ç–∞–Ω–Ω—è **–Ω–µ –ø–æ–∫—Ä–∏–≤–∞—î—Ç—å—Å—è –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏**, —Å–∏—Å—Ç–µ–º–∞ –ø–æ–≤—ñ–¥–æ–º–∏—Ç—å: **‚Äú–ù–µ–º–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.‚Äù**
""".strip()


def on_provider_change(provider: str):
    cfg = PROVIDERS.get(provider, PROVIDERS["Groq"])
    # base_url visible —Ç—ñ–ª—å–∫–∏ –¥–ª—è Custom
    base_visible = (provider == "Custom")
    return (
        gr.update(value=cfg["base_url"], visible=base_visible),
        gr.update(value=cfg["model"]),
        gr.update(value=cfg["note"])
    )


def ask(question, use_bm25, use_dense, api_key, provider, base_url, model):
    try:
        # —è–∫—â–æ –Ω–µ Custom ‚Äî –±–µ—Ä–µ–º–æ base_url –∑ provider –∫–æ–Ω—Ñ—ñ–≥—ñ–≤
        if provider in PROVIDERS and provider != "Custom":
            base_url = PROVIDERS[provider]["base_url"]

        answer, sources = rag.answer(
            question=question,
            use_bm25=use_bm25,
            use_dense=use_dense,
            api_key=api_key,
            base_url=base_url,
            model=model
        )

        src_text = "\n".join([f"[{i+1}] {s}" for i, s in enumerate(sources)])
        return answer, src_text
    except Exception as e:
        return f"‚ùå –ü–æ–º–∏–ª–∫–∞: {str(e)}", ""


with gr.Blocks(title="RAG NLP QA") as demo:
    gr.Markdown(DOMAIN_TEXT)

    question = gr.Textbox(
        label="Question",
        placeholder="–ù–∞–ø—Ä–∏–∫–ª–∞–¥: –ü–æ—è—Å–Ω–∏ self-attention: —â–æ —Ç–∞–∫–µ Q, K, V?"
    )

    with gr.Row():
        use_bm25 = gr.Checkbox(label="BM25", value=True)
        use_dense = gr.Checkbox(label="Semantic", value=True)

    gr.Markdown("### LLM settings")
    api_key = gr.Textbox(label="API key", type="password")

    provider = gr.Dropdown(
        label="Provider",
        choices=["Groq", "OpenAI", "Custom"],
        value="Groq"
    )

    base_url = gr.Textbox(
        label="Base URL (—Ç—ñ–ª—å–∫–∏ –¥–ª—è Custom)",
        value=PROVIDERS["Groq"]["base_url"],
        visible=False
    )

    model = gr.Textbox(label="Model", value=PROVIDERS["Groq"]["model"])
    provider_note = gr.Markdown(PROVIDERS["Groq"]["note"])

    answer = gr.Textbox(label="Answer", lines=6)
    sources = gr.Textbox(label="Sources", lines=6)

    btn = gr.Button("Ask")

    provider.change(
        on_provider_change,
        inputs=[provider],
        outputs=[base_url, model, provider_note]
    )

    btn.click(
        ask,
        inputs=[question, use_bm25, use_dense, api_key, provider, base_url, model],
        outputs=[answer, sources]
    )

if __name__ == "__main__":
    demo.launch()
