# Transformers and Attention

Transformers are neural network architectures based on the self-attention mechanism.

## Self-attention
Self-attention allows each token to attend to all other tokens in the sequence.
It uses queries (Q), keys (K), and values (V) to compute attention scores.

## Advantages over RNNs
Transformers allow parallel computation and handle long-range dependencies
better than recurrent neural networks.
